{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0% 0/2500 [00:03<?, ?it/s, lr=0.01, epoch=0, loss=0.703, fake_loss=0.708, real_loss=0.698]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0:   0% 1/2500 [00:07<5:28:28,  7.89s/it, lr=0.01, epoch=0, loss=0.703, fake_loss=0.708, real_loss=0.698]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0: 100% 2499/2500 [1:10:18<00:01,  1.69s/it, lr=0.00978, epoch=0, loss=0.517, fake_loss=0.521, real_loss=0.514]\n",
      "Epoch 0: 100% 2499/2500 [1:10:18<00:01,  1.69s/it, lr=0.00978, epoch=0, loss=0.524, fake_loss=0.523, real_loss=0.524]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 1: 100% 2499/2500 [1:10:44<00:01,  1.70s/it, lr=0.00955, epoch=1, loss=0.354, fake_loss=0.362, real_loss=0.346]\n",
      "Epoch 1: 100% 2499/2500 [1:24:11<00:02,  2.02s/it, lr=0.00955, epoch=1, loss=0.351, fake_loss=0.355, real_loss=0.346]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 2: 100% 2499/2500 [1:10:25<00:01,  1.69s/it, lr=0.00933, epoch=2, loss=0.304, fake_loss=0.312, real_loss=0.296]\n",
      "Epoch 2: 100% 2499/2500 [1:23:54<00:02,  2.01s/it, lr=0.00933, epoch=2, loss=0.305, fake_loss=0.315, real_loss=0.296]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:37<00:00,  3.72it/s]\n",
      "Epoch 3: 100% 2499/2500 [1:10:35<00:01,  1.69s/it, lr=0.0091, epoch=3, loss=0.277, fake_loss=0.284, real_loss=0.271] \n",
      "Epoch 3: 100% 2499/2500 [1:24:15<00:02,  2.02s/it, lr=0.0091, epoch=3, loss=0.274, fake_loss=0.28, real_loss=0.268]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:32<00:00,  3.74it/s]\n",
      "Epoch 4: 100% 2499/2500 [1:10:43<00:01,  1.70s/it, lr=0.00887, epoch=4, loss=0.259, fake_loss=0.268, real_loss=0.251]\n",
      "Epoch 4: 100% 2499/2500 [1:24:19<00:02,  2.02s/it, lr=0.00887, epoch=4, loss=0.253, fake_loss=0.257, real_loss=0.248]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:29<00:00,  3.75it/s]\n",
      "Epoch 5: 100% 2499/2500 [1:10:46<00:01,  1.70s/it, lr=0.00865, epoch=5, loss=0.242, fake_loss=0.251, real_loss=0.234]\n",
      "Epoch 5: 100% 2499/2500 [1:24:19<00:02,  2.02s/it, lr=0.00865, epoch=5, loss=0.239, fake_loss=0.246, real_loss=0.232]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:34<00:00,  3.73it/s]\n",
      "Epoch 6: 100% 2499/2500 [1:24:31<00:02,  2.03s/it, lr=0.00842, epoch=6, loss=0.236, fake_loss=0.246, real_loss=0.226]\n",
      "Epoch 6: 100% 2499/2500 [1:10:52<00:01,  1.70s/it, lr=0.00842, epoch=6, loss=0.231, fake_loss=0.239, real_loss=0.224]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:40<00:00,  3.71it/s]\n",
      "Epoch 7: 100% 2499/2500 [1:24:29<00:02,  2.03s/it, lr=0.00819, epoch=7, loss=0.227, fake_loss=0.234, real_loss=0.22]]\n",
      "Epoch 7: 100% 2499/2500 [1:10:45<00:01,  1.70s/it, lr=0.00819, epoch=7, loss=0.225, fake_loss=0.234, real_loss=0.217]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 8: 100% 2499/2500 [1:10:45<00:01,  1.70s/it, lr=0.00796, epoch=8, loss=0.222, fake_loss=0.229, real_loss=0.215] \n",
      "Epoch 8: 100% 2499/2500 [1:24:25<00:02,  2.03s/it, lr=0.00796, epoch=8, loss=0.218, fake_loss=0.227, real_loss=0.209]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:30<00:00,  3.75it/s]\n",
      "Epoch 9: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00773, epoch=9, loss=0.214, fake_loss=0.223, real_loss=0.206]\n",
      "Epoch 9: 100% 2499/2500 [1:24:15<00:02,  2.02s/it, lr=0.00773, epoch=9, loss=0.214, fake_loss=0.223, real_loss=0.204]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:32<00:00,  3.74it/s]\n",
      "Epoch 10: 100% 2499/2500 [1:24:49<00:02,  2.04s/it, lr=0.0075, epoch=10, loss=0.207, fake_loss=0.212, real_loss=0.203] \n",
      "Epoch 10: 100% 2499/2500 [1:11:12<00:01,  1.71s/it, lr=0.0075, epoch=10, loss=0.208, fake_loss=0.214, real_loss=0.202]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:31<00:00,  3.74it/s]\n",
      "Epoch 11: 100% 2499/2500 [1:24:36<00:02,  2.03s/it, lr=0.00727, epoch=11, loss=0.204, fake_loss=0.21, real_loss=0.197] \n",
      "Epoch 11: 100% 2499/2500 [1:10:59<00:01,  1.70s/it, lr=0.00727, epoch=11, loss=0.207, fake_loss=0.213, real_loss=0.2]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 12: 100% 2499/2500 [1:10:43<00:01,  1.70s/it, lr=0.00704, epoch=12, loss=0.205, fake_loss=0.213, real_loss=0.196]   \n",
      "Epoch 12: 100% 2499/2500 [1:24:21<00:02,  2.03s/it, lr=0.00704, epoch=12, loss=0.203, fake_loss=0.209, real_loss=0.197]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:29<00:00,  3.76it/s]\n",
      "Epoch 13: 100% 2499/2500 [1:10:52<00:01,  1.70s/it, lr=0.0068, epoch=13, loss=0.201, fake_loss=0.209, real_loss=0.194]  \n",
      "Epoch 13: 100% 2499/2500 [1:24:24<00:02,  2.03s/it, lr=0.0068, epoch=13, loss=0.196, fake_loss=0.201, real_loss=0.191]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.78it/s]\n",
      "Epoch 14: 100% 2499/2500 [1:10:30<00:01,  1.69s/it, lr=0.00657, epoch=14, loss=0.199, fake_loss=0.208, real_loss=0.19]]\n",
      "Epoch 14: 100% 2499/2500 [1:23:56<00:02,  2.02s/it, lr=0.00657, epoch=14, loss=0.193, fake_loss=0.204, real_loss=0.182]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 15: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.00633, epoch=15, loss=0.195, fake_loss=0.204, real_loss=0.187] \n",
      "Epoch 15: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00633, epoch=15, loss=0.197, fake_loss=0.206, real_loss=0.188]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 16: 100% 2499/2500 [1:10:34<00:01,  1.69s/it, lr=0.0061, epoch=16, loss=0.195, fake_loss=0.205, real_loss=0.185] \n",
      "Epoch 16: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=0.0061, epoch=16, loss=0.191, fake_loss=0.203, real_loss=0.179]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 17: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.00586, epoch=17, loss=0.193, fake_loss=0.204, real_loss=0.182]  \n",
      "Epoch 17: 100% 2499/2500 [1:24:11<00:02,  2.02s/it, lr=0.00586, epoch=17, loss=0.19, fake_loss=0.196, real_loss=0.184]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:32<00:00,  3.74it/s]\n",
      "Epoch 18: 100% 2499/2500 [1:10:39<00:01,  1.70s/it, lr=0.00562, epoch=18, loss=0.191, fake_loss=0.196, real_loss=0.185] \n",
      "Epoch 18: 100% 2499/2500 [1:24:14<00:02,  2.02s/it, lr=0.00562, epoch=18, loss=0.189, fake_loss=0.196, real_loss=0.182]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:27<00:00,  3.77it/s]\n",
      "Epoch 19: 100% 2499/2500 [1:10:38<00:01,  1.70s/it, lr=0.00538, epoch=19, loss=0.186, fake_loss=0.2, real_loss=0.172]8]\n",
      "Epoch 19: 100% 2499/2500 [1:24:08<00:02,  2.02s/it, lr=0.00538, epoch=19, loss=0.189, fake_loss=0.201, real_loss=0.178]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 20: 100% 2499/2500 [1:10:30<00:01,  1.69s/it, lr=0.00514, epoch=20, loss=0.188, fake_loss=0.202, real_loss=0.174]   \n",
      "Epoch 20: 100% 2499/2500 [1:24:01<00:02,  2.02s/it, lr=0.00514, epoch=20, loss=0.189, fake_loss=0.197, real_loss=0.181]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 21: 100% 2499/2500 [1:10:13<00:01,  1.69s/it, lr=0.0049, epoch=21, loss=0.188, fake_loss=0.2, real_loss=0.175]4]  \n",
      "Epoch 21: 100% 2499/2500 [1:23:39<00:02,  2.01s/it, lr=0.0049, epoch=21, loss=0.188, fake_loss=0.201, real_loss=0.174]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:25<00:00,  3.77it/s]\n",
      "Epoch 22: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00466, epoch=22, loss=0.183, fake_loss=0.196, real_loss=0.17]]   lr=0.0047, epoch=22, loss=0.183, fake_loss=0.195, real_loss=0.17] \n",
      "Epoch 22: 100% 2499/2500 [1:24:10<00:02,  2.02s/it, lr=0.00466, epoch=22, loss=0.181, fake_loss=0.187, real_loss=0.174]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 23: 100% 2499/2500 [1:10:35<00:01,  1.69s/it, lr=0.00441, epoch=23, loss=0.185, fake_loss=0.195, real_loss=0.176]   \n",
      "Epoch 23: 100% 2499/2500 [1:23:59<00:02,  2.02s/it, lr=0.00441, epoch=23, loss=0.183, fake_loss=0.196, real_loss=0.171]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 24: 100% 2499/2500 [1:10:45<00:01,  1.70s/it, lr=0.00417, epoch=24, loss=0.18, fake_loss=0.192, real_loss=0.168]] \n",
      "Epoch 24: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00417, epoch=24, loss=0.181, fake_loss=0.191, real_loss=0.172]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 25: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.00392, epoch=25, loss=0.181, fake_loss=0.189, real_loss=0.174]  \n",
      "Epoch 25: 100% 2499/2500 [1:23:58<00:02,  2.02s/it, lr=0.00392, epoch=25, loss=0.178, fake_loss=0.184, real_loss=0.171]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.79it/s]\n",
      "Epoch 26: 100% 2499/2500 [1:10:39<00:01,  1.70s/it, lr=0.00367, epoch=26, loss=0.18, fake_loss=0.189, real_loss=0.17]]  it, lr=0.00379, epoch=26, loss=0.183, fake_loss=0.193, real_loss=0.174]\n",
      "Epoch 26: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=0.00367, epoch=26, loss=0.18, fake_loss=0.187, real_loss=0.173]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:17<00:00,  3.81it/s]\n",
      "Epoch 27: 100% 2499/2500 [1:11:01<00:01,  1.71s/it, lr=0.00342, epoch=27, loss=0.179, fake_loss=0.188, real_loss=0.169]  \n",
      "Epoch 27: 100% 2499/2500 [1:24:22<00:02,  2.03s/it, lr=0.00342, epoch=27, loss=0.18, fake_loss=0.189, real_loss=0.172]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 28: 100% 2499/2500 [1:11:02<00:01,  1.71s/it, lr=0.00317, epoch=28, loss=0.175, fake_loss=0.18, real_loss=0.169]] \n",
      "Epoch 28: 100% 2499/2500 [1:24:26<00:02,  2.03s/it, lr=0.00317, epoch=28, loss=0.176, fake_loss=0.181, real_loss=0.171]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 29: 100% 2499/2500 [1:10:48<00:01,  1.70s/it, lr=0.00291, epoch=29, loss=0.177, fake_loss=0.191, real_loss=0.164]   \n",
      "Epoch 29: 100% 2499/2500 [1:24:10<00:02,  2.02s/it, lr=0.00291, epoch=29, loss=0.177, fake_loss=0.189, real_loss=0.164]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:31<00:00,  3.74it/s]\n",
      "Epoch 30: 100% 2499/2500 [1:10:37<00:01,  1.70s/it, lr=0.00265, epoch=30, loss=0.17, fake_loss=0.18, real_loss=0.16]5]  \n",
      "Epoch 30: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00265, epoch=30, loss=0.17, fake_loss=0.174, real_loss=0.165]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 31: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00239, epoch=31, loss=0.171, fake_loss=0.177, real_loss=0.165] \n",
      "Epoch 31: 100% 2499/2500 [1:24:07<00:02,  2.02s/it, lr=0.00239, epoch=31, loss=0.167, fake_loss=0.179, real_loss=0.155]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.79it/s]\n",
      "Epoch 32: 100% 2499/2500 [1:24:11<00:02,  2.02s/it, lr=0.00213, epoch=32, loss=0.171, fake_loss=0.183, real_loss=0.16]]   \n",
      "Epoch 32: 100% 2499/2500 [1:10:46<00:01,  1.70s/it, lr=0.00213, epoch=32, loss=0.169, fake_loss=0.176, real_loss=0.162]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:25<00:00,  3.77it/s]\n",
      "Epoch 33: 100% 2499/2500 [1:10:31<00:01,  1.69s/it, lr=0.00186, epoch=33, loss=0.166, fake_loss=0.174, real_loss=0.158] \n",
      "Epoch 33: 100% 2499/2500 [1:23:59<00:02,  2.02s/it, lr=0.00186, epoch=33, loss=0.168, fake_loss=0.178, real_loss=0.158]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 34: 100% 2499/2500 [1:24:13<00:02,  2.02s/it, lr=0.00159, epoch=34, loss=0.17, fake_loss=0.18, real_loss=0.161]]  \n",
      "Epoch 34: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00159, epoch=34, loss=0.17, fake_loss=0.187, real_loss=0.154]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 35: 100% 2499/2500 [1:10:36<00:01,  1.70s/it, lr=0.00131, epoch=35, loss=0.164, fake_loss=0.171, real_loss=0.157]\n",
      "Epoch 35: 100% 2499/2500 [1:23:59<00:02,  2.02s/it, lr=0.00131, epoch=35, loss=0.164, fake_loss=0.174, real_loss=0.155]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 36: 100% 2499/2500 [1:24:06<00:02,  2.02s/it, lr=0.00103, epoch=36, loss=0.16, fake_loss=0.17, real_loss=0.151]5] \n",
      "Epoch 36: 100% 2499/2500 [1:10:39<00:01,  1.70s/it, lr=0.00103, epoch=36, loss=0.164, fake_loss=0.173, real_loss=0.155]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 37: 100% 2499/2500 [1:10:54<00:01,  1.70s/it, lr=0.000732, epoch=37, loss=0.164, fake_loss=0.177, real_loss=0.15]  \n",
      "Epoch 37: 100% 2499/2500 [1:24:17<00:02,  2.02s/it, lr=0.000732, epoch=37, loss=0.162, fake_loss=0.17, real_loss=0.154]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 38: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.000424, epoch=38, loss=0.162, fake_loss=0.169, real_loss=0.155] \n",
      "Epoch 38: 100% 2499/2500 [1:24:03<00:02,  2.02s/it, lr=0.000424, epoch=38, loss=0.159, fake_loss=0.166, real_loss=0.151]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 39: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=8.46e-5, epoch=39, loss=0.161, fake_loss=0.175, real_loss=0.147]    \n",
      "Epoch 39: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=8.46e-5, epoch=39, loss=0.16, fake_loss=0.17, real_loss=0.149]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:37<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -u -m torch.distributed.launch --nproc_per_node=2 --master_port 9901 training/pipelines/train_classifier.py \\\n",
    " --distributed --config configs/b7.json --freeze-epochs 0 --test_every 1 --opt-level O1 --label-smoothing 0.01 --folds-csv folds.csv   --fold 0 --seed 111 --data-dir ../dfdc_train_all --prefix b7_111_ > logs/b7_111\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0% 0/2500 [00:05<?, ?it/s, lr=0.01, epoch=0, loss=0.676, fake_loss=0.616, real_loss=0.737]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0:   0% 1/2500 [00:14<10:14:46, 14.76s/it, lr=0.01, epoch=0, loss=0.676, fake_loss=0.616, real_loss=0.737]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0: 100% 2499/2500 [1:10:50<00:01,  1.70s/it, lr=0.00978, epoch=0, loss=0.51, fake_loss=0.511, real_loss=0.509] \n",
      "Epoch 0: 100% 2499/2500 [1:10:50<00:01,  1.70s/it, lr=0.00978, epoch=0, loss=0.513, fake_loss=0.515, real_loss=0.51]\n",
      "100%|███████████████████████████████████████| 3040/3040 [14:04<00:00,  3.60it/s]\n",
      "Epoch 1: 100% 2499/2500 [1:10:26<00:01,  1.69s/it, lr=0.00955, epoch=1, loss=0.347, fake_loss=0.35, real_loss=0.344]]\n",
      "Epoch 1: 100% 2499/2500 [1:24:39<00:02,  2.03s/it, lr=0.00955, epoch=1, loss=0.352, fake_loss=0.361, real_loss=0.343]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:56<00:00,  3.63it/s]\n",
      "Epoch 2: 100% 2499/2500 [1:10:00<00:01,  1.68s/it, lr=0.00933, epoch=2, loss=0.303, fake_loss=0.309, real_loss=0.297]\n",
      "Epoch 2: 100% 2499/2500 [1:23:58<00:02,  2.02s/it, lr=0.00933, epoch=2, loss=0.293, fake_loss=0.299, real_loss=0.287]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:57<00:00,  3.63it/s]\n",
      "Epoch 3: 100% 2499/2500 [1:10:30<00:01,  1.69s/it, lr=0.0091, epoch=3, loss=0.269, fake_loss=0.271, real_loss=0.266] \n",
      "Epoch 3: 100% 2499/2500 [1:24:32<00:02,  2.03s/it, lr=0.0091, epoch=3, loss=0.274, fake_loss=0.277, real_loss=0.271]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:55<00:00,  3.64it/s]\n",
      "Epoch 4: 100% 2499/2500 [1:10:45<00:01,  1.70s/it, lr=0.00887, epoch=4, loss=0.252, fake_loss=0.259, real_loss=0.246] 485/2500 [13:40<56:28,  1.68s/it, lr=0.00906, epoch=4, loss=0.261, fake_loss=0.272, real_loss=0.25]\n",
      "Epoch 4: 100% 2499/2500 [1:24:44<00:02,  2.03s/it, lr=0.00887, epoch=4, loss=0.252, fake_loss=0.257, real_loss=0.246]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:51<00:00,  3.66it/s]\n",
      "Epoch 5: 100% 2499/2500 [1:11:14<00:01,  1.71s/it, lr=0.00865, epoch=5, loss=0.244, fake_loss=0.251, real_loss=0.238]\n",
      "Epoch 5: 100% 2499/2500 [1:25:08<00:02,  2.04s/it, lr=0.00865, epoch=5, loss=0.24, fake_loss=0.248, real_loss=0.232]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:51<00:00,  3.66it/s]\n",
      "Epoch 6: 100% 2499/2500 [1:11:04<00:01,  1.71s/it, lr=0.00842, epoch=6, loss=0.233, fake_loss=0.24, real_loss=0.226]]\n",
      "Epoch 6: 100% 2499/2500 [1:24:59<00:02,  2.04s/it, lr=0.00842, epoch=6, loss=0.235, fake_loss=0.242, real_loss=0.228]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:45<00:00,  3.68it/s]\n",
      "Epoch 7: 100% 2499/2500 [1:11:10<00:01,  1.71s/it, lr=0.00819, epoch=7, loss=0.224, fake_loss=0.232, real_loss=0.217]\n",
      "Epoch 7: 100% 2499/2500 [1:25:00<00:02,  2.04s/it, lr=0.00819, epoch=7, loss=0.222, fake_loss=0.23, real_loss=0.215]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:47<00:00,  3.67it/s]\n",
      "Epoch 8: 100% 2499/2500 [1:11:19<00:01,  1.71s/it, lr=0.00796, epoch=8, loss=0.219, fake_loss=0.229, real_loss=0.209]\n",
      "Epoch 8: 100% 2499/2500 [1:25:08<00:02,  2.04s/it, lr=0.00796, epoch=8, loss=0.218, fake_loss=0.223, real_loss=0.212]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:42<00:00,  3.70it/s]\n",
      "Epoch 9: 100% 2499/2500 [1:11:09<00:01,  1.71s/it, lr=0.00773, epoch=9, loss=0.215, fake_loss=0.219, real_loss=0.21]]\n",
      "Epoch 9: 100% 2499/2500 [1:24:53<00:02,  2.04s/it, lr=0.00773, epoch=9, loss=0.212, fake_loss=0.223, real_loss=0.201]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:43<00:00,  3.69it/s]\n",
      "Epoch 10: 100% 2499/2500 [1:11:35<00:01,  1.72s/it, lr=0.0075, epoch=10, loss=0.21, fake_loss=0.222, real_loss=0.198]   \n",
      "Epoch 10: 100% 2499/2500 [1:25:22<00:02,  2.05s/it, lr=0.0075, epoch=10, loss=0.21, fake_loss=0.219, real_loss=0.201]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:42<00:00,  3.70it/s]\n",
      "Epoch 11: 100% 2499/2500 [1:11:43<00:01,  1.72s/it, lr=0.00727, epoch=11, loss=0.209, fake_loss=0.22, real_loss=0.198] \n",
      "Epoch 11: 100% 2499/2500 [1:25:27<00:02,  2.05s/it, lr=0.00727, epoch=11, loss=0.2, fake_loss=0.205, real_loss=0.195]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:43<00:00,  3.69it/s]\n",
      "Epoch 12: 100% 2499/2500 [1:11:51<00:01,  1.73s/it, lr=0.00704, epoch=12, loss=0.206, fake_loss=0.215, real_loss=0.198]   \n",
      "Epoch 12: 100% 2499/2500 [1:25:37<00:02,  2.06s/it, lr=0.00704, epoch=12, loss=0.204, fake_loss=0.211, real_loss=0.197]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:38<00:00,  3.71it/s]\n",
      "Epoch 13: 100% 2499/2500 [1:11:51<00:01,  1.73s/it, lr=0.0068, epoch=13, loss=0.201, fake_loss=0.208, real_loss=0.194]  \n",
      "Epoch 13: 100% 2499/2500 [1:25:32<00:02,  2.05s/it, lr=0.0068, epoch=13, loss=0.201, fake_loss=0.212, real_loss=0.189]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:44<00:00,  3.69it/s]\n",
      "Epoch 14: 100% 2499/2500 [1:12:13<00:01,  1.73s/it, lr=0.00657, epoch=14, loss=0.197, fake_loss=0.204, real_loss=0.19]] \n",
      "Epoch 14: 100% 2499/2500 [1:26:00<00:02,  2.07s/it, lr=0.00657, epoch=14, loss=0.192, fake_loss=0.204, real_loss=0.181]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:34<00:00,  3.73it/s]\n",
      "Epoch 15: 100% 2499/2500 [1:12:21<00:01,  1.74s/it, lr=0.00633, epoch=15, loss=0.194, fake_loss=0.202, real_loss=0.186]  \n",
      "Epoch 15: 100% 2499/2500 [1:25:59<00:02,  2.06s/it, lr=0.00633, epoch=15, loss=0.195, fake_loss=0.203, real_loss=0.187]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:34<00:00,  3.73it/s]\n",
      "Epoch 16: 100% 2499/2500 [1:12:25<00:01,  1.74s/it, lr=0.0061, epoch=16, loss=0.193, fake_loss=0.202, real_loss=0.183]  \n",
      "Epoch 16: 100% 2499/2500 [1:26:03<00:02,  2.07s/it, lr=0.0061, epoch=16, loss=0.193, fake_loss=0.203, real_loss=0.183]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:27<00:00,  3.77it/s]\n",
      "Epoch 17: 100% 2499/2500 [1:12:22<00:01,  1.74s/it, lr=0.00586, epoch=17, loss=0.188, fake_loss=0.195, real_loss=0.181] \n",
      "Epoch 17: 100% 2499/2500 [1:25:52<00:02,  2.06s/it, lr=0.00586, epoch=17, loss=0.195, fake_loss=0.207, real_loss=0.184]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:26<00:00,  3.77it/s]\n",
      "Epoch 18: 100% 2499/2500 [1:12:27<00:01,  1.74s/it, lr=0.00562, epoch=18, loss=0.188, fake_loss=0.194, real_loss=0.181] \n",
      "Epoch 18: 100% 2499/2500 [1:25:56<00:02,  2.06s/it, lr=0.00562, epoch=18, loss=0.189, fake_loss=0.195, real_loss=0.184]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 19: 100% 2499/2500 [1:12:59<00:01,  1.75s/it, lr=0.00538, epoch=19, loss=0.188, fake_loss=0.199, real_loss=0.178]\n",
      "Epoch 19: 100% 2499/2500 [1:26:23<00:02,  2.07s/it, lr=0.00538, epoch=19, loss=0.189, fake_loss=0.199, real_loss=0.179]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 20: 100% 2499/2500 [1:12:52<00:01,  1.75s/it, lr=0.00514, epoch=20, loss=0.187, fake_loss=0.198, real_loss=0.177] \n",
      "Epoch 20: 100% 2499/2500 [1:26:20<00:02,  2.07s/it, lr=0.00514, epoch=20, loss=0.187, fake_loss=0.194, real_loss=0.18]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 21: 100% 2499/2500 [1:13:20<00:01,  1.76s/it, lr=0.0049, epoch=21, loss=0.187, fake_loss=0.194, real_loss=0.181]  \n",
      "Epoch 21: 100% 2499/2500 [1:26:44<00:02,  2.08s/it, lr=0.0049, epoch=21, loss=0.187, fake_loss=0.195, real_loss=0.179]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 22: 100% 2499/2500 [1:13:38<00:01,  1.77s/it, lr=0.00466, epoch=22, loss=0.186, fake_loss=0.193, real_loss=0.18]   \n",
      "Epoch 22: 100% 2499/2500 [1:27:02<00:02,  2.09s/it, lr=0.00466, epoch=22, loss=0.181, fake_loss=0.192, real_loss=0.17]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:16<00:00,  3.82it/s]\n",
      "Epoch 23: 100% 2499/2500 [1:13:42<00:01,  1.77s/it, lr=0.00441, epoch=23, loss=0.183, fake_loss=0.191, real_loss=0.175] \n",
      "Epoch 23: 100% 2499/2500 [1:27:00<00:02,  2.09s/it, lr=0.00441, epoch=23, loss=0.184, fake_loss=0.192, real_loss=0.176]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:11<00:00,  3.84it/s]\n",
      "Epoch 24: 100% 2499/2500 [1:13:40<00:01,  1.77s/it, lr=0.00417, epoch=24, loss=0.178, fake_loss=0.185, real_loss=0.171]\n",
      "Epoch 24: 100% 2499/2500 [1:26:56<00:02,  2.09s/it, lr=0.00417, epoch=24, loss=0.18, fake_loss=0.189, real_loss=0.17]\n",
      " 87%|██████████████████████████████████     | 2659/3040 [11:37<01:37,  3.92it/s]"
     ]
    }
   ],
   "source": [
    "!python -u -m torch.distributed.launch --nproc_per_node=2 --master_port 9901 training/pipelines/train_classifier.py \\\n",
    " --distributed --config configs/b7.json --freeze-epochs 0 --test_every 1 --opt-level O1 --label-smoothing 0.01 --folds-csv folds.csv  --fold 0 --seed 555 --data-dir ../dfdc_train_all --prefix b7_555_ > logs/b7_555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u -m torch.distributed.launch --nproc_per_node=2 --master_port 9901 training/pipelines/train_classifier.py \\\n",
    " --distributed --config configs/b7.json --freeze-epochs 0 --test_every 1 --opt-level O1 --label-smoothing 0.01 --folds-csv folds.csv  --fold 0 --seed 777 --data-dir ../dfdc_train_all --prefix b7_777_ > logs/b7_777\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
