{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0% 0/2500 [00:04<?, ?it/s, lr=0.01, epoch=0, loss=0.691, fake_loss=0.696, real_loss=0.686]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0:   0% 1/2500 [00:10<7:19:51, 10.56s/it, lr=0.01, epoch=0, loss=0.704, fake_loss=0.711, real_loss=0.697]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0: 100% 2499/2500 [1:10:17<00:01,  1.69s/it, lr=0.00978, epoch=0, loss=0.515, fake_loss=0.518, real_loss=0.512]\n",
      "Epoch 0: 100% 2499/2500 [1:10:17<00:01,  1.69s/it, lr=0.00978, epoch=0, loss=0.518, fake_loss=0.517, real_loss=0.52]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.78it/s]\n",
      "Epoch 1: 100% 2499/2500 [1:24:08<00:02,  2.02s/it, lr=0.00955, epoch=1, loss=0.358, fake_loss=0.363, real_loss=0.353]\n",
      "Epoch 1: 100% 2499/2500 [1:10:41<00:01,  1.70s/it, lr=0.00955, epoch=1, loss=0.355, fake_loss=0.36, real_loss=0.349]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 2: 100% 2499/2500 [1:23:54<00:02,  2.01s/it, lr=0.00933, epoch=2, loss=0.303, fake_loss=0.312, real_loss=0.294]\n",
      "Epoch 2: 100% 2499/2500 [1:10:23<00:01,  1.69s/it, lr=0.00933, epoch=2, loss=0.299, fake_loss=0.303, real_loss=0.296]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:37<00:00,  3.72it/s]\n",
      "Epoch 3: 100% 2499/2500 [1:24:15<00:02,  2.02s/it, lr=0.0091, epoch=3, loss=0.275, fake_loss=0.284, real_loss=0.265]    \n",
      "Epoch 3: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.0091, epoch=3, loss=0.269, fake_loss=0.278, real_loss=0.261]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:32<00:00,  3.74it/s]\n",
      "Epoch 4: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00887, epoch=4, loss=0.252, fake_loss=0.261, real_loss=0.243]\n",
      "Epoch 4: 100% 2499/2500 [1:24:19<00:02,  2.02s/it, lr=0.00887, epoch=4, loss=0.258, fake_loss=0.269, real_loss=0.248]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:30<00:00,  3.75it/s]\n",
      "Epoch 5: 100% 2499/2500 [1:24:19<00:02,  2.02s/it, lr=0.00865, epoch=5, loss=0.242, fake_loss=0.252, real_loss=0.231] \n",
      "Epoch 5: 100% 2499/2500 [1:10:46<00:01,  1.70s/it, lr=0.00865, epoch=5, loss=0.241, fake_loss=0.247, real_loss=0.234]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:33<00:00,  3.74it/s]\n",
      "Epoch 6: 100% 2499/2500 [1:24:30<00:02,  2.03s/it, lr=0.00842, epoch=6, loss=0.235, fake_loss=0.242, real_loss=0.228]\n",
      "Epoch 6: 100% 2499/2500 [1:10:54<00:01,  1.70s/it, lr=0.00842, epoch=6, loss=0.231, fake_loss=0.239, real_loss=0.224]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:39<00:00,  3.71it/s]\n",
      "Epoch 7: 100% 2499/2500 [1:10:45<00:01,  1.70s/it, lr=0.00819, epoch=7, loss=0.227, fake_loss=0.236, real_loss=0.218] \n",
      "Epoch 7: 100% 2499/2500 [1:24:29<00:02,  2.03s/it, lr=0.00819, epoch=7, loss=0.227, fake_loss=0.232, real_loss=0.223]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 8: 100% 2499/2500 [1:24:25<00:02,  2.03s/it, lr=0.00796, epoch=8, loss=0.218, fake_loss=0.223, real_loss=0.213]\n",
      "Epoch 8: 100% 2499/2500 [1:10:46<00:01,  1.70s/it, lr=0.00796, epoch=8, loss=0.223, fake_loss=0.232, real_loss=0.214]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 9: 100% 2499/2500 [1:24:15<00:02,  2.02s/it, lr=0.00773, epoch=9, loss=0.214, fake_loss=0.223, real_loss=0.206]   \n",
      "Epoch 9: 100% 2499/2500 [1:10:44<00:01,  1.70s/it, lr=0.00773, epoch=9, loss=0.21, fake_loss=0.221, real_loss=0.199]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:32<00:00,  3.74it/s]\n",
      "Epoch 10: 100% 2499/2500 [1:24:49<00:02,  2.04s/it, lr=0.0075, epoch=10, loss=0.207, fake_loss=0.214, real_loss=0.2]]     \n",
      "Epoch 10: 100% 2499/2500 [1:11:14<00:01,  1.71s/it, lr=0.0075, epoch=10, loss=0.21, fake_loss=0.217, real_loss=0.203]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:31<00:00,  3.74it/s]\n",
      "Epoch 11: 100% 2499/2500 [1:24:36<00:02,  2.03s/it, lr=0.00727, epoch=11, loss=0.206, fake_loss=0.215, real_loss=0.198]\n",
      "Epoch 11: 100% 2499/2500 [1:11:00<00:01,  1.70s/it, lr=0.00727, epoch=11, loss=0.208, fake_loss=0.217, real_loss=0.199]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:36<00:00,  3.72it/s]\n",
      "Epoch 12: 100% 2499/2500 [1:24:22<00:02,  2.03s/it, lr=0.00704, epoch=12, loss=0.198, fake_loss=0.205, real_loss=0.191] \n",
      "Epoch 12: 100% 2499/2500 [1:10:41<00:01,  1.70s/it, lr=0.00704, epoch=12, loss=0.205, fake_loss=0.212, real_loss=0.198]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:29<00:00,  3.75it/s]\n",
      "Epoch 13: 100% 2499/2500 [1:24:24<00:02,  2.03s/it, lr=0.0068, epoch=13, loss=0.199, fake_loss=0.215, real_loss=0.182]   \n",
      "Epoch 13: 100% 2499/2500 [1:10:52<00:01,  1.70s/it, lr=0.0068, epoch=13, loss=0.197, fake_loss=0.201, real_loss=0.194]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.78it/s]\n",
      "Epoch 14: 100% 2499/2500 [1:23:56<00:02,  2.02s/it, lr=0.00657, epoch=14, loss=0.197, fake_loss=0.207, real_loss=0.187]\n",
      "Epoch 14: 100% 2499/2500 [1:10:30<00:01,  1.69s/it, lr=0.00657, epoch=14, loss=0.198, fake_loss=0.2, real_loss=0.197]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 15: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00633, epoch=15, loss=0.197, fake_loss=0.207, real_loss=0.187] \n",
      "Epoch 15: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.00633, epoch=15, loss=0.193, fake_loss=0.2, real_loss=0.185]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:27<00:00,  3.76it/s]\n",
      "Epoch 16: 100% 2499/2500 [1:10:34<00:01,  1.69s/it, lr=0.0061, epoch=16, loss=0.197, fake_loss=0.209, real_loss=0.185] \n",
      "Epoch 16: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=0.0061, epoch=16, loss=0.195, fake_loss=0.204, real_loss=0.186]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:35<00:00,  3.73it/s]\n",
      "Epoch 17: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00586, epoch=17, loss=0.193, fake_loss=0.2, real_loss=0.186]8]\n",
      "\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:31<00:00,  3.74it/s]\n",
      "Epoch 18: 100% 2499/2500 [1:24:14<00:02,  2.02s/it, lr=0.00562, epoch=18, loss=0.191, fake_loss=0.199, real_loss=0.184] \n",
      "Epoch 18: 100% 2499/2500 [1:10:39<00:01,  1.70s/it, lr=0.00562, epoch=18, loss=0.191, fake_loss=0.197, real_loss=0.185]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:27<00:00,  3.77it/s]\n",
      "Epoch 19: 100% 2499/2500 [1:10:38<00:01,  1.70s/it, lr=0.00538, epoch=19, loss=0.187, fake_loss=0.195, real_loss=0.18]] \n",
      "Epoch 19: 100% 2499/2500 [1:24:08<00:02,  2.02s/it, lr=0.00538, epoch=19, loss=0.184, fake_loss=0.192, real_loss=0.175]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 20: 100% 2499/2500 [1:10:30<00:01,  1.69s/it, lr=0.00514, epoch=20, loss=0.185, fake_loss=0.192, real_loss=0.179]  \n",
      "Epoch 20: 100% 2499/2500 [1:24:01<00:02,  2.02s/it, lr=0.00514, epoch=20, loss=0.192, fake_loss=0.202, real_loss=0.181]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.79it/s]\n",
      "Epoch 21: 100% 2499/2500 [1:23:39<00:02,  2.01s/it, lr=0.0049, epoch=21, loss=0.184, fake_loss=0.195, real_loss=0.173]  \n",
      "Epoch 21: 100% 2499/2500 [1:10:13<00:01,  1.69s/it, lr=0.0049, epoch=21, loss=0.186, fake_loss=0.197, real_loss=0.176]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:26<00:00,  3.77it/s]\n",
      "Epoch 22: 100% 2499/2500 [1:24:10<00:02,  2.02s/it, lr=0.00466, epoch=22, loss=0.186, fake_loss=0.191, real_loss=0.181]\n",
      "Epoch 22: 100% 2499/2500 [1:10:41<00:01,  1.70s/it, lr=0.00466, epoch=22, loss=0.184, fake_loss=0.196, real_loss=0.173]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 23: 100% 2499/2500 [1:10:34<00:01,  1.69s/it, lr=0.00441, epoch=23, loss=0.178, fake_loss=0.186, real_loss=0.169]  46, epoch=23, loss=0.18, fake_loss=0.186, real_loss=0.173]\n",
      "Epoch 23: 100% 2499/2500 [1:23:59<00:02,  2.02s/it, lr=0.00441, epoch=23, loss=0.183, fake_loss=0.193, real_loss=0.172]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:25<00:00,  3.78it/s]\n",
      "Epoch 24: 100% 2499/2500 [1:24:13<00:02,  2.02s/it, lr=0.00417, epoch=24, loss=0.183, fake_loss=0.196, real_loss=0.17]]   00418, epoch=24, loss=0.178, fake_loss=0.188, real_loss=0.168]\n",
      "Epoch 24: 100% 2499/2500 [1:10:43<00:01,  1.70s/it, lr=0.00417, epoch=24, loss=0.178, fake_loss=0.187, real_loss=0.169]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 25: 100% 2499/2500 [1:10:33<00:01,  1.69s/it, lr=0.00392, epoch=25, loss=0.182, fake_loss=0.19, real_loss=0.173] \n",
      "Epoch 25: 100% 2499/2500 [1:23:57<00:02,  2.02s/it, lr=0.00392, epoch=25, loss=0.177, fake_loss=0.184, real_loss=0.17]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 26: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=0.00367, epoch=26, loss=0.181, fake_loss=0.192, real_loss=0.169]  \n",
      "Epoch 26: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.00367, epoch=26, loss=0.178, fake_loss=0.187, real_loss=0.168]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:17<00:00,  3.81it/s]\n",
      "Epoch 27: 100% 2499/2500 [1:11:02<00:01,  1.71s/it, lr=0.00342, epoch=27, loss=0.174, fake_loss=0.183, real_loss=0.164]\n",
      "Epoch 27: 100% 2499/2500 [1:24:22<00:02,  2.03s/it, lr=0.00342, epoch=27, loss=0.177, fake_loss=0.187, real_loss=0.167]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 28: 100% 2499/2500 [1:11:02<00:01,  1.71s/it, lr=0.00317, epoch=28, loss=0.18, fake_loss=0.188, real_loss=0.171]] \n",
      "Epoch 28: 100% 2499/2500 [1:24:26<00:02,  2.03s/it, lr=0.00317, epoch=28, loss=0.177, fake_loss=0.187, real_loss=0.167]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 29: 100% 2499/2500 [1:10:48<00:01,  1.70s/it, lr=0.00291, epoch=29, loss=0.172, fake_loss=0.182, real_loss=0.162]\n",
      "Epoch 29: 100% 2499/2500 [1:24:10<00:02,  2.02s/it, lr=0.00291, epoch=29, loss=0.173, fake_loss=0.186, real_loss=0.16]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:31<00:00,  3.74it/s]\n",
      "Epoch 30: 100% 2499/2500 [1:10:37<00:01,  1.70s/it, lr=0.00265, epoch=30, loss=0.174, fake_loss=0.18, real_loss=0.167]] \n",
      "Epoch 30: 100% 2499/2500 [1:24:12<00:02,  2.02s/it, lr=0.00265, epoch=30, loss=0.173, fake_loss=0.184, real_loss=0.162]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:21<00:00,  3.79it/s]\n",
      "Epoch 31: 100% 2499/2500 [1:10:42<00:01,  1.70s/it, lr=0.00239, epoch=31, loss=0.17, fake_loss=0.178, real_loss=0.162]   \n",
      "Epoch 31: 100% 2499/2500 [1:24:06<00:02,  2.02s/it, lr=0.00239, epoch=31, loss=0.17, fake_loss=0.18, real_loss=0.16]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 32: 100% 2499/2500 [1:24:11<00:02,  2.02s/it, lr=0.00213, epoch=32, loss=0.167, fake_loss=0.173, real_loss=0.16]    \n",
      "Epoch 32: 100% 2499/2500 [1:10:46<00:01,  1.70s/it, lr=0.00213, epoch=32, loss=0.171, fake_loss=0.18, real_loss=0.161]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:25<00:00,  3.77it/s]\n",
      "Epoch 33: 100% 2499/2500 [1:24:00<00:02,  2.02s/it, lr=0.00186, epoch=33, loss=0.167, fake_loss=0.18, real_loss=0.155]]   \n",
      "Epoch 33: 100% 2499/2500 [1:10:31<00:01,  1.69s/it, lr=0.00186, epoch=33, loss=0.164, fake_loss=0.172, real_loss=0.157]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:28<00:00,  3.76it/s]\n",
      "Epoch 34: 100% 2499/2500 [1:24:13<00:02,  2.02s/it, lr=0.00159, epoch=34, loss=0.169, fake_loss=0.178, real_loss=0.161]  \n",
      "Epoch 34: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.00159, epoch=34, loss=0.168, fake_loss=0.181, real_loss=0.155]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 35: 100% 2499/2500 [1:23:59<00:02,  2.02s/it, lr=0.00131, epoch=35, loss=0.162, fake_loss=0.173, real_loss=0.151]\n",
      "Epoch 35: 100% 2499/2500 [1:10:36<00:01,  1.70s/it, lr=0.00131, epoch=35, loss=0.167, fake_loss=0.177, real_loss=0.156]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:24<00:00,  3.78it/s]\n",
      "Epoch 36: 100% 2499/2500 [1:10:39<00:01,  1.70s/it, lr=0.00103, epoch=36, loss=0.166, fake_loss=0.174, real_loss=0.157]  \n",
      "Epoch 36: 100% 2499/2500 [1:24:06<00:02,  2.02s/it, lr=0.00103, epoch=36, loss=0.166, fake_loss=0.179, real_loss=0.153]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:20<00:00,  3.80it/s]\n",
      "Epoch 37: 100% 2499/2500 [1:24:18<00:02,  2.02s/it, lr=0.000732, epoch=37, loss=0.161, fake_loss=0.171, real_loss=0.152]  \n",
      "Epoch 37: 100% 2499/2500 [1:10:53<00:01,  1.70s/it, lr=0.000732, epoch=37, loss=0.161, fake_loss=0.17, real_loss=0.151]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 38: 100% 2499/2500 [1:24:02<00:02,  2.02s/it, lr=0.000424, epoch=38, loss=0.162, fake_loss=0.169, real_loss=0.154]]\n",
      "Epoch 38: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.000424, epoch=38, loss=0.165, fake_loss=0.179, real_loss=0.151]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 39: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=8.46e-5, epoch=39, loss=0.158, fake_loss=0.166, real_loss=0.151]    \n",
      "Epoch 39: 100% 2499/2500 [1:24:05<00:02,  2.02s/it, lr=8.46e-5, epoch=39, loss=0.162, fake_loss=0.172, real_loss=0.152]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:37<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -u -m torch.distributed.launch --nproc_per_node=2 --master_port 9902 training/pipelines/train_classifier.py \\\n",
    " --distributed --config configs/b7.json --freeze-epochs 0 --test_every 1 --opt-level O1 --label-smoothing 0.01 --folds-csv folds.csv  --fold 0 --seed 888 --data-dir ../dfdc_train_all --prefix b7_888_ > logs/b7_888\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0% 0/2500 [00:05<?, ?it/s, lr=0.01, epoch=0, loss=0.686, fake_loss=0.747, real_loss=0.624]/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 0: 100% 2499/2500 [1:10:50<00:01,  1.70s/it, lr=0.00978, epoch=0, loss=0.514, fake_loss=0.518, real_loss=0.511]\n",
      "Epoch 0: 100% 2499/2500 [1:10:50<00:01,  1.70s/it, lr=0.00978, epoch=0, loss=0.515, fake_loss=0.514, real_loss=0.515]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:40<00:00,  3.71it/s]\n",
      "Epoch 1: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.00955, epoch=1, loss=0.352, fake_loss=0.361, real_loss=0.343]\n",
      "Epoch 1: 100% 2499/2500 [1:24:24<00:02,  2.03s/it, lr=0.00955, epoch=1, loss=0.362, fake_loss=0.369, real_loss=0.355]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:29<00:00,  3.76it/s]\n",
      "Epoch 2: 100% 2499/2500 [1:10:11<00:01,  1.69s/it, lr=0.00933, epoch=2, loss=0.299, fake_loss=0.303, real_loss=0.294] \n",
      "Epoch 2: 100% 2499/2500 [1:23:43<00:02,  2.01s/it, lr=0.00933, epoch=2, loss=0.297, fake_loss=0.306, real_loss=0.288]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:25<00:00,  3.77it/s]\n",
      "Epoch 3: 100% 2499/2500 [1:10:40<00:01,  1.70s/it, lr=0.0091, epoch=3, loss=0.279, fake_loss=0.288, real_loss=0.269] \n",
      "Epoch 3: 100% 2499/2500 [1:24:08<00:02,  2.02s/it, lr=0.0091, epoch=3, loss=0.276, fake_loss=0.283, real_loss=0.269]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:26<00:00,  3.77it/s]\n",
      "Epoch 4: 100% 2499/2500 [1:10:58<00:01,  1.70s/it, lr=0.00887, epoch=4, loss=0.255, fake_loss=0.263, real_loss=0.247]\n",
      "Epoch 4: 100% 2499/2500 [1:24:29<00:02,  2.03s/it, lr=0.00887, epoch=4, loss=0.258, fake_loss=0.262, real_loss=0.254]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:22<00:00,  3.79it/s]\n",
      "Epoch 5: 100% 2499/2500 [1:11:27<00:01,  1.72s/it, lr=0.00865, epoch=5, loss=0.242, fake_loss=0.253, real_loss=0.231] \n",
      "Epoch 5: 100% 2499/2500 [1:24:53<00:02,  2.04s/it, lr=0.00865, epoch=5, loss=0.244, fake_loss=0.249, real_loss=0.239]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:23<00:00,  3.78it/s]\n",
      "Epoch 6: 100% 2499/2500 [1:11:16<00:01,  1.71s/it, lr=0.00842, epoch=6, loss=0.232, fake_loss=0.238, real_loss=0.227]\n",
      "Epoch 6: 100% 2499/2500 [1:24:42<00:02,  2.03s/it, lr=0.00842, epoch=6, loss=0.241, fake_loss=0.253, real_loss=0.228]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:19<00:00,  3.80it/s]\n",
      "Epoch 7: 100% 2499/2500 [1:11:21<00:01,  1.71s/it, lr=0.00819, epoch=7, loss=0.227, fake_loss=0.237, real_loss=0.217]\n",
      "Epoch 7: 100% 2499/2500 [1:24:45<00:02,  2.04s/it, lr=0.00819, epoch=7, loss=0.227, fake_loss=0.237, real_loss=0.218]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:16<00:00,  3.82it/s]\n",
      "Epoch 8: 100% 2499/2500 [1:11:29<00:01,  1.72s/it, lr=0.00796, epoch=8, loss=0.222, fake_loss=0.224, real_loss=0.219] \n",
      "Epoch 8: 100% 2499/2500 [1:24:48<00:02,  2.04s/it, lr=0.00796, epoch=8, loss=0.217, fake_loss=0.222, real_loss=0.212]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:14<00:00,  3.82it/s]\n",
      "Epoch 9: 100% 2499/2500 [1:11:22<00:01,  1.71s/it, lr=0.00773, epoch=9, loss=0.211, fake_loss=0.22, real_loss=0.202]]\n",
      "Epoch 9: 100% 2499/2500 [1:24:41<00:02,  2.03s/it, lr=0.00773, epoch=9, loss=0.214, fake_loss=0.227, real_loss=0.201]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:13<00:00,  3.83it/s]\n",
      "Epoch 10: 100% 2499/2500 [1:11:48<00:01,  1.72s/it, lr=0.0075, epoch=10, loss=0.209, fake_loss=0.213, real_loss=0.205] \n",
      "Epoch 10: 100% 2499/2500 [1:25:05<00:02,  2.04s/it, lr=0.0075, epoch=10, loss=0.207, fake_loss=0.213, real_loss=0.201]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:12<00:00,  3.84it/s]\n",
      "Epoch 11: 100% 2499/2500 [1:25:10<00:02,  2.04s/it, lr=0.00727, epoch=11, loss=0.206, fake_loss=0.217, real_loss=0.195]\n",
      "Epoch 11: 100% 2499/2500 [1:11:55<00:01,  1.73s/it, lr=0.00727, epoch=11, loss=0.21, fake_loss=0.22, real_loss=0.199]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:10<00:00,  3.85it/s]\n",
      "Epoch 12: 100% 2499/2500 [1:12:04<00:01,  1.73s/it, lr=0.00704, epoch=12, loss=0.206, fake_loss=0.215, real_loss=0.198] \n",
      "Epoch 12: 100% 2499/2500 [1:25:17<00:02,  2.05s/it, lr=0.00704, epoch=12, loss=0.202, fake_loss=0.211, real_loss=0.193]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:09<00:00,  3.85it/s]\n",
      "Epoch 13: 100% 2499/2500 [1:25:14<00:02,  2.05s/it, lr=0.0068, epoch=13, loss=0.201, fake_loss=0.208, real_loss=0.194] \n",
      "Epoch 13: 100% 2499/2500 [1:12:02<00:01,  1.73s/it, lr=0.0068, epoch=13, loss=0.2, fake_loss=0.21, real_loss=0.19]\n",
      "100%|███████████████████████████████████████| 3040/3040 [13:02<00:00,  3.89it/s]\n",
      "Epoch 14: 100% 2499/2500 [1:25:28<00:02,  2.05s/it, lr=0.00657, epoch=14, loss=0.199, fake_loss=0.208, real_loss=0.19]]\n",
      "Epoch 14: 100% 2499/2500 [1:12:23<00:01,  1.74s/it, lr=0.00657, epoch=14, loss=0.197, fake_loss=0.202, real_loss=0.193]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:59<00:00,  3.90it/s]\n",
      "Epoch 15: 100% 2499/2500 [1:12:35<00:01,  1.74s/it, lr=0.00633, epoch=15, loss=0.195, fake_loss=0.206, real_loss=0.184]  \n",
      "Epoch 15: 100% 2499/2500 [1:25:37<00:02,  2.06s/it, lr=0.00633, epoch=15, loss=0.196, fake_loss=0.2, real_loss=0.192]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:55<00:00,  3.92it/s]\n",
      "Epoch 16: 100% 2499/2500 [1:12:36<00:01,  1.74s/it, lr=0.0061, epoch=16, loss=0.191, fake_loss=0.199, real_loss=0.184]  \n",
      "Epoch 16: 100% 2499/2500 [1:25:34<00:02,  2.05s/it, lr=0.0061, epoch=16, loss=0.194, fake_loss=0.206, real_loss=0.183]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:53<00:00,  3.93it/s]\n",
      "Epoch 17: 100% 2499/2500 [1:25:31<00:02,  2.05s/it, lr=0.00586, epoch=17, loss=0.188, fake_loss=0.197, real_loss=0.18]]\n",
      "Epoch 17: 100% 2499/2500 [1:12:35<00:01,  1.74s/it, lr=0.00586, epoch=17, loss=0.199, fake_loss=0.212, real_loss=0.186]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:52<00:00,  3.94it/s]\n",
      "Epoch 18: 100% 2499/2500 [1:12:38<00:01,  1.74s/it, lr=0.00562, epoch=18, loss=0.186, fake_loss=0.192, real_loss=0.179] \n",
      "Epoch 18: 100% 2499/2500 [1:25:33<00:02,  2.05s/it, lr=0.00562, epoch=18, loss=0.189, fake_loss=0.195, real_loss=0.183]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:48<00:00,  3.96it/s]\n",
      "Epoch 19: 100% 2499/2500 [1:26:05<00:02,  2.07s/it, lr=0.00538, epoch=19, loss=0.189, fake_loss=0.196, real_loss=0.182] \n",
      "Epoch 19: 100% 2499/2500 [1:13:12<00:01,  1.76s/it, lr=0.00538, epoch=19, loss=0.188, fake_loss=0.199, real_loss=0.177]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:50<00:00,  3.95it/s]\n",
      "Epoch 20: 100% 2499/2500 [1:13:08<00:01,  1.76s/it, lr=0.00514, epoch=20, loss=0.182, fake_loss=0.189, real_loss=0.176]   \n",
      "Epoch 20: 100% 2499/2500 [1:26:00<00:02,  2.06s/it, lr=0.00514, epoch=20, loss=0.182, fake_loss=0.191, real_loss=0.172]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:45<00:00,  3.97it/s]\n",
      "Epoch 21: 100% 2499/2500 [1:13:32<00:01,  1.77s/it, lr=0.0049, epoch=21, loss=0.187, fake_loss=0.197, real_loss=0.177]  \n",
      "Epoch 21: 100% 2499/2500 [1:26:20<00:02,  2.07s/it, lr=0.0049, epoch=21, loss=0.183, fake_loss=0.19, real_loss=0.176]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:42<00:00,  3.99it/s]\n",
      "Epoch 22: 100% 2499/2500 [1:26:36<00:02,  2.08s/it, lr=0.00466, epoch=22, loss=0.181, fake_loss=0.191, real_loss=0.172]\n",
      "Epoch 22: 100% 2499/2500 [1:13:51<00:01,  1.77s/it, lr=0.00466, epoch=22, loss=0.181, fake_loss=0.186, real_loss=0.175]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:40<00:00,  4.00it/s]\n",
      "Epoch 23: 100% 2499/2500 [1:13:59<00:01,  1.78s/it, lr=0.00441, epoch=23, loss=0.182, fake_loss=0.189, real_loss=0.175]   \n",
      "Epoch 23: 100% 2499/2500 [1:26:42<00:02,  2.08s/it, lr=0.00441, epoch=23, loss=0.181, fake_loss=0.192, real_loss=0.17]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:39<00:00,  4.01it/s]\n",
      "Epoch 24: 100% 2499/2500 [1:13:51<00:01,  1.77s/it, lr=0.00417, epoch=24, loss=0.18, fake_loss=0.189, real_loss=0.17]]  \n",
      "Epoch 24: 100% 2499/2500 [1:26:33<00:02,  2.08s/it, lr=0.00417, epoch=24, loss=0.177, fake_loss=0.183, real_loss=0.17]\n",
      "100%|███████████████████████████████████████| 3040/3040 [12:36<00:00,  4.02it/s]\n",
      "Epoch 25:   6% 142/2500 [06:49<1:51:47,  2.84s/it, lr=0.00415, epoch=25, loss=0.202, fake_loss=0.216, real_loss=0.188]    "
     ]
    }
   ],
   "source": [
    "!python -u -m torch.distributed.launch --nproc_per_node=2 --master_port 9902 training/pipelines/train_classifier.py \\\n",
    " --distributed --config configs/b7.json --freeze-epochs 0 --test_every 1 --opt-level O1 --label-smoothing 0.01 --folds-csv folds.csv  --fold 0 --seed 999 --data-dir ../dfdc_train_all --prefix b7_999_ > logs/b7_999\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
