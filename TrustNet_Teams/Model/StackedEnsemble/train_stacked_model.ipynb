{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "from pipeline.metrics import accuracy_b\n",
    "from pipeline.model_methods import validate, train\n",
    "from pipeline.data_loaders import load_img_dataset, load_img_val_dataset\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import timm\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, models, device='cuda:0', extended=False):\n",
    "        super(MetaModel, self).__init__()\n",
    "        \n",
    "        self.extended = extended\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        self.len = len(models)\n",
    "        \n",
    "        if self.extended:\n",
    "            self.bn = nn.BatchNorm1d(self.len)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(self.len, 1)\n",
    "        \n",
    "    def optimized_forward(self, x):\n",
    "        x = torch.cat(tuple(x), dim=1)\n",
    "        \n",
    "        if self.extended:\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            #x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat(tuple([model(x) for model in self.models]), dim=1)\n",
    "        \n",
    "        if self.extended:\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_grad(model):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "    return model\n",
    "\n",
    "def validate(model, x_val, batch_size, checkpoint=0.31):\n",
    "    val_loss = []\n",
    "    dataloader_iterator = iter(x_val)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm.tqdm_notebook(range(len(x_val))):\n",
    "            try:\n",
    "                X_batch, y_batch, _ = next(dataloader_iterator)\n",
    "            except:\n",
    "                dataloader_iterator = iter(x_val)\n",
    "                X_batch, y_batch, _ = next(dataloader_iterator)\n",
    "\n",
    "            y_batch = torch.FloatTensor(y_batch).to(device)\n",
    "            X_batch = X_batch.to(device)\n",
    "\n",
    "            stacked_preds = []\n",
    "            preds = []\n",
    "            \n",
    "            '''\n",
    "            for i in range(len(stack_models)):\n",
    "                stacked_preds.append(stack_models[i](X_batch))\n",
    "\n",
    "            for i in range(len(models)):\n",
    "                preds.append(models[i].optimized_forward(stacked_preds[meta_models[i][1]]))\n",
    "            '''\n",
    "                \n",
    "            preds = model(X_batch)\n",
    "            \n",
    "            test_loss_value = F.binary_cross_entropy_with_logits(preds, y_batch).item() * batch_size\n",
    "            #test_loss_value /= len(models)\n",
    "\n",
    "            val_loss.append(test_loss_value)\n",
    "\n",
    "        mean_loss = sum(val_loss) / (len(x_val) * batch_size)\n",
    "        \n",
    "        if mean_loss <= checkpoint:\n",
    "            torch.save(model.state_dict(), model.__class__.__name__  + ' ' + str(mean_loss) + '.pth')\n",
    "\n",
    "        print('Validation: ', mean_loss)\n",
    "        \n",
    "        return mean_loss\n",
    "        \n",
    "def train(model, x_train, x_val, optimizer, scheduler, batch_size, epochs=10):\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in tqdm.tqdm_notebook(range(epochs)):\n",
    "        dataloader_iterator = iter(x_train)\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = []\n",
    "\n",
    "        for batch_idx in tqdm.tqdm_notebook(range(len(x_train))):   \n",
    "            try:\n",
    "                X_batch, y_batch, _ = next(dataloader_iterator)\n",
    "            except:\n",
    "                dataloader_iterator = iter(x_train)\n",
    "                X_batch, y_batch, _ = next(dataloader_iterator)\n",
    "\n",
    "            y_batch = torch.FloatTensor(y_batch).to(device)\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            stacked_preds = []\n",
    "            preds = []\n",
    "            \n",
    "            '''\n",
    "            for i in range(len(stack_models)):\n",
    "                stacked_preds.append(stack_models[i](X_batch))\n",
    "\n",
    "            for i in range(len(models)):\n",
    "                preds.append(models[i].optimized_forward(stacked_preds[meta_models[i][1]]))\n",
    "            '''\n",
    "                \n",
    "            preds = model(X_batch)\n",
    "\n",
    "            loss_value = F.binary_cross_entropy_with_logits(preds, y_batch)\n",
    "            loss_value.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "            train_loss.append(loss_value.item() * batch_size)\n",
    "\n",
    "        test_loss_value = validate(model, x_val, batch_size)\n",
    "\n",
    "        mean_loss = sum(train_loss) / (len(x_train) * batch_size)\n",
    "        \n",
    "        print('Train: ', mean_loss)\n",
    "        print('Epoch:', epoch+1)\n",
    "\n",
    "        test_loss_history.append(test_loss_value)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return test_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 19154)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv(r'data\\metadata.csv')\n",
    "len(y_train[y_train.label == 1]), len(y_train[y_train.label == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: /workspace/dataset\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8bbbd6a2495a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img_val_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/workspace/dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/workspace/dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# r'data\\img_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/dfdc/dfdc-kaggle-solution/pipeline/data_loaders.py\u001b[0m in \u001b[0;36mload_img_val_dataset\u001b[0;34m(data_path, batch_size, resize)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             ),\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             ])\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n\u001b[0;32m---> 97\u001b[0;31m                                 \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /workspace/dataset\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "x_val = load_img_val_dataset('/workspace/dataset', batch_size)\n",
    "x_train = load_img_dataset('/workspace/dataset', batch_size, resize=256, crop=None, num_samples=None)\n",
    "\n",
    "# r'data\\img_val\n",
    "# r'data\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "\n",
    "models = []\n",
    "weights = []\n",
    "\n",
    "raw_models = \\\n",
    "[\n",
    "    ['pretrained/efficientnetb2 0.8548137313946486 0.3376769562025044.pth', 'efficientnet-b2'],\n",
    "    ['pretrained/EfficientNetb3 0.8573518024606384 0.34558522378585194.pth', 'efficientnet-b3'],\n",
    "    ['pretrained/EfficientNetb4 0.8579110384582294 0.3383911053075265.pth', 'efficientnet-b4'],\n",
    "    ['pretrained/EfficientNet6 0.8602770369095758 0.33193617861157143.pth', 'efficientnet-b6'],\n",
    "    \n",
    "    ['pretrained/EfficientNetb0 t2 0.8616966359803837 0.3698434531609828.pth', 'efficientnet-b0'],\n",
    "    ['pretrained/EfficientNetb1 t2 0.8410909403768391 0.36058002083572327.pth', 'efficientnet-b1'],\n",
    "    ['pretrained/EfficientNetb2 t2 0.8659554331928073 0.35598630783834084.pth', 'efficientnet-b2'],\n",
    "    ['pretrained/EfficientNetb3 t2 0.8486191172674868 0.3611779548592305.pth', 'efficientnet-b3'],\n",
    "    \n",
    "    ['pretrained/EfficientNetb3 0.8635894347414609 0.328333642473084.pth', 'efficientnet-b3'],\n",
    "    ['pretrained/EfficientNetb6 0.8593736556826981 0.32286693639934694.pth', 'efficientnet-b6'],\n",
    "    \n",
    "    ['pretrained/tf_efficientnet_b1_ns 0.8571367116923342 0.3341234226295108.pth', 'tf_efficientnet_b1_ns'],\n",
    "    ['pretrained/tf_efficientnet_b3_ns 0.8712466660930913 0.3277394129117183.pth', 'tf_efficientnet_b3_ns'],\n",
    "    ['pretrained/tf_efficientnet_b4_ns 0.8708595027101437 0.3152573955405342.pth', 'tf_efficientnet_b4_ns'],\n",
    "    ['pretrained/tf_efficientnet_b6_ns 0.8733115374688118 0.3156576980666498.pth', 'tf_efficientnet_b6_ns'],\n",
    "]\n",
    "\n",
    "meta_models = \\\n",
    "[\n",
    "    ['pretrained/MetaModel 0.30638167556896007.pth', slice(4, 8), False],\n",
    "    ['pretrained/MetaModel 0.2919331893755284.pth', slice(0, 4), False],\n",
    "    ['pretrained/MetaModel 0.30281482560578044.pth', slice(0, 8, None), True],\n",
    "    ['pretrained/MetaModel 0.26302117601197256.pth', slice(0, 10, None), False],\n",
    "    ['pretrained/MetaModel 0.264787397152165.pth', slice(0, 14, None), False]\n",
    "]\n",
    "\n",
    "\n",
    "stack_models = []\n",
    "for raw_model in raw_models:\n",
    "    checkpoint = torch.load(raw_model[0], map_location=device)\n",
    "    \n",
    "    if '-' in raw_model[1]:\n",
    "        model = EfficientNet.from_name(raw_model[1])\n",
    "        model._fc = nn.Linear(model._fc.in_features, 1)\n",
    "    else:\n",
    "        model = timm.create_model(raw_model[1], pretrained=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model = model.to(device)\n",
    "    stack_models.append(model)\n",
    "\n",
    "    del checkpoint, model\n",
    "    \n",
    "for meta_raw in meta_models:\n",
    "\n",
    "    checkpoint = torch.load(meta_raw[0], map_location=device)\n",
    "    model = MetaModel(models=raw_models[meta_raw[1]], extended=meta_raw[2]).to(device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "    del model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "model = MetaModel(stack_models).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.) \n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(x_train), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, x_train, x_val, optimizer, scheduler, batch_size, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
