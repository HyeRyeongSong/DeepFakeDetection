{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from PIL import ImageFilter, Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import timm\n",
    "from imutils.video import FileVideoStream \n",
    "from efficientnet_pytorch import EfficientNet #efficientnet (X)\n",
    "\n",
    "sys.path.append('./inference/helpers')\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = \"/workspace/dataset/FAKE\"\n",
    "\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "test_videos = test_videos[:300]\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.4.0\n",
      "CUDA version: 10.1\n",
      "cuDNN version: 7603\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"./inference/helpers/blazeface.pth\")\n",
    "facedet.load_anchors(\"./inference/helpers/anchors.npy\")\n",
    "_ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_grad(model):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "    return model\n",
    "        \n",
    "\n",
    "def weight_preds(preds, weights):\n",
    "    final_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            if len(final_preds) != len(preds[i]):\n",
    "                final_preds.append(preds[i][j] * weights[i])\n",
    "            else:\n",
    "                final_preds[j] += preds[i][j] * weights[i]\n",
    "                \n",
    "    return torch.FloatTensor(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_video_1 import VideoReader\n",
    "from face_extract_1 import FaceExtractor\n",
    "\n",
    "frames_per_video = 120\n",
    "\n",
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video) #get_frames(x, batch_size=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, models=None, device='cuda:0', extended=False):\n",
    "        super(MetaModel, self).__init__()\n",
    "        \n",
    "        self.extended = extended\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        self.len = len(models)\n",
    "        \n",
    "        if self.extended:\n",
    "            self.bn = nn.BatchNorm1d(self.len)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(self.len, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat(tuple(x), dim=1)\n",
    "        \n",
    "        if self.extended:\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            #x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = \"./inference/pretrained/\"\n",
    "WEIGTHS_EXT = '.pth'\n",
    "\n",
    "models = []\n",
    "weigths = []\n",
    "    \n",
    "raw_data_stack = \\\n",
    "[\n",
    "    ['0.8548137313946486 0.3376769562025044', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 0.8573518024606384 0.34558522378585194', 'efficientnet-b3'],\n",
    "    ['EfficientNetb4 0.8579110384582294 0.3383911053075265', 'efficientnet-b4'],\n",
    "    ['EfficientNet6 0.8602770369095758 0.33193617861157143', 'efficientnet-b6'],\n",
    "    ['EfficientNetb0 t2 0.8616966359803837 0.3698434531609828', 'efficientnet-b0'],\n",
    "    ['EfficientNetb1 t2 0.8410909403768391 0.36058002083572327', 'efficientnet-b1'],\n",
    "    ['EfficientNetb2 t2 0.8659554331928073 0.35598630783834084', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 t2 0.8486191172674868 0.3611779548592305', 'efficientnet-b3'],\n",
    "    ['EfficientNetb3 0.8635894347414609 0.328333642473084', 'efficientnet-b3'],\n",
    "    ['EfficientNetb6 0.8593736556826981 0.32286693639934694', 'efficientnet-b6'],\n",
    "    \n",
    "    ['tf_efficientnet_b1_ns 0.8571367116923342 0.3341234226295108', 'tf_efficientnet_b1_ns'],\n",
    "    ['tf_efficientnet_b3_ns 0.8712466660930913 0.3277394129117183', 'tf_efficientnet_b3_ns'],\n",
    "    ['tf_efficientnet_b4_ns 0.8708595027101437 0.3152573955405342', 'tf_efficientnet_b4_ns'],\n",
    "    ['tf_efficientnet_b6_ns 0.8733115374688118 0.3156576980666498', 'tf_efficientnet_b6_ns'],\n",
    "]\n",
    "\n",
    "stack_models = []\n",
    "\n",
    "for raw_model in raw_data_stack:\n",
    "    checkpoint = torch.load( MODELS_PATH + raw_model[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    if '-' in raw_model[1]:\n",
    "        model = EfficientNet.from_name(raw_model[1])\n",
    "        model._fc = nn.Linear(model._fc.in_features, 1)\n",
    "    else:\n",
    "        model = timm.create_model(raw_model[1], pretrained=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model = model.to(device)\n",
    "    stack_models.append(model)\n",
    "\n",
    "    del checkpoint, model\n",
    "    \n",
    "\n",
    "meta_models = \\\n",
    "[\n",
    "    ['MetaModel 0.30638167556896007', slice(4, 8), False, 0.37780],\n",
    "    ['MetaModel 0.2919331893755284', slice(0, 4), False, 0.33357],\n",
    "    ['MetaModel 0.30281482560578044', slice(0, 8, None), True, 0.34077],\n",
    "    ['MetaModel 0.26302117601197256', slice(0, 10, None), False, 0.35134],\n",
    "    ['MetaModel 0.256337642808031', slice(10, 14, None), False, 0.32698],\n",
    "]\n",
    "\n",
    "for meta_raw in meta_models:\n",
    "\n",
    "    checkpoint = torch.load(MODELS_PATH + meta_raw[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    model = MetaModel(models=raw_data_stack[meta_raw[1]], extended=meta_raw[2]).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    weigths.append(meta_raw[3])\n",
    "\n",
    "    del model, checkpoint\n",
    "    \n",
    "total = sum([1-score for score in weigths])\n",
    "weigths = [(1-score) / total for score in weigths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    resized_face = cv2.resize(face, (input_size, input_size))\n",
    "                    \n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "            del faces\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction\n",
    "                with torch.no_grad():\n",
    "                    y_pred = 0\n",
    "                    stacked_preds = []\n",
    "                    preds = []\n",
    "                    \n",
    "                    for i in range(len(stack_models)):\n",
    "                        stacked_preds.append(stack_models[i](x).squeeze()[:n].unsqueeze(dim=1))\n",
    "                    \n",
    "                    for i in range(len(models)):\n",
    "                        preds.append(models[i](stacked_preds[meta_models[i][1]]))\n",
    "                \n",
    "                    del x, stacked_preds\n",
    "                    \n",
    "                    y_pred = torch.sigmoid(weight_preds(preds, weigths)).mean().item()\n",
    "                    \n",
    "                    del preds\n",
    "                    \n",
    "                    return y_pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "    \n",
    "    \n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "        \n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
    "submission_df.to_csv(\"predection_fake.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaagqkcdis.mp4</td>\n",
       "      <td>0.998312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaoqepxnf.mp4</td>\n",
       "      <td>0.799491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaavbjopeq.mp4</td>\n",
       "      <td>0.993602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaavflklag.mp4</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aabdnomlru.mp4</td>\n",
       "      <td>0.999370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>abzkaeazro.mp4</td>\n",
       "      <td>0.990113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>acagallncj.mp4</td>\n",
       "      <td>0.907798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>acajitpqte.mp4</td>\n",
       "      <td>0.997952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>acanmekatk.mp4</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.848187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename     label\n",
       "0    aaagqkcdis.mp4  0.998312\n",
       "1    aaaoqepxnf.mp4  0.799491\n",
       "2    aaavbjopeq.mp4  0.993602\n",
       "3    aaavflklag.mp4  0.999000\n",
       "4    aabdnomlru.mp4  0.999370\n",
       "..              ...       ...\n",
       "295  abzkaeazro.mp4  0.990113\n",
       "296  acagallncj.mp4  0.907798\n",
       "297  acajitpqte.mp4  0.997952\n",
       "298  acanmekatk.mp4  0.500000\n",
       "299  acazlolrpz.mp4  0.848187\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readcsv = pd.read_csv(\"./predection_fake.csv\")\n",
    "readcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(readcsv)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(limit):\n",
    "    return sum([1 for _ in readcsv.label if _ >= limit]) / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90%  62.33333333333333\n",
      "80%  76.66666666666667\n",
      "70%  85.33333333333334\n",
      "60%  90.0\n",
      "51%  92.0\n"
     ]
    }
   ],
   "source": [
    "print(\"90% \", score(0.9))\n",
    "print(\"80% \", score(0.8))\n",
    "print(\"70% \", score(0.7))\n",
    "print(\"60% \", score(0.6))\n",
    "print(\"51% \", score(0.51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
